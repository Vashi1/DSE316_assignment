{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all Modules and importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import SVHN\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Load SVHN dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = SVHN(root='./data', split='train', transform=transform, download=True)\n",
    "test_dataset = SVHN(root='./data', split='test', transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, _ = train_test_split(train_dataset, test_size=0.75, random_state=42)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LeCun Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dse313/miniconda3/envs/dse316_assignment/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dse313/miniconda3/envs/dse316_assignment/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'LeNet-5': torchvision.models.resnet18(pretrained=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LeNet-5...\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    model.fc = nn.Linear(512, num_classes)\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"Training {model_name}...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the LeNet-5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LeNet-5: 89.98540258143824%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of {model_name}: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the AlexNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dse313/miniconda3/envs/dse316_assignment/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1145], Loss: 2.218105666637421\n",
      "Epoch [1/10], Step [200/1145], Loss: 2.110550717115402\n",
      "Epoch [1/10], Step [300/1145], Loss: 1.951042778491974\n",
      "Epoch [1/10], Step [400/1145], Loss: 1.7350592195987702\n",
      "Epoch [1/10], Step [500/1145], Loss: 1.5222872257232667\n",
      "Epoch [1/10], Step [600/1145], Loss: 1.4065490019321443\n",
      "Epoch [1/10], Step [700/1145], Loss: 1.2889359718561173\n",
      "Epoch [1/10], Step [800/1145], Loss: 1.2421942168474198\n",
      "Epoch [1/10], Step [900/1145], Loss: 1.181700549721718\n",
      "Epoch [1/10], Step [1000/1145], Loss: 1.1523692524433136\n",
      "Epoch [1/10], Step [1100/1145], Loss: 1.095138350725174\n",
      "Epoch [2/10], Step [100/1145], Loss: 1.0461846941709518\n",
      "Epoch [2/10], Step [200/1145], Loss: 1.029409232735634\n",
      "Epoch [2/10], Step [300/1145], Loss: 1.0402674329280854\n",
      "Epoch [2/10], Step [400/1145], Loss: 1.0287544041872025\n",
      "Epoch [2/10], Step [500/1145], Loss: 0.9794797849655151\n",
      "Epoch [2/10], Step [600/1145], Loss: 1.009885831475258\n",
      "Epoch [2/10], Step [700/1145], Loss: 0.9953354287147522\n",
      "Epoch [2/10], Step [800/1145], Loss: 0.9693287941813469\n",
      "Epoch [2/10], Step [900/1145], Loss: 0.9379022365808487\n",
      "Epoch [2/10], Step [1000/1145], Loss: 0.9409301149845123\n",
      "Epoch [2/10], Step [1100/1145], Loss: 0.9368175899982453\n",
      "Epoch [3/10], Step [100/1145], Loss: 0.9226963061094284\n",
      "Epoch [3/10], Step [200/1145], Loss: 0.9188472193479538\n",
      "Epoch [3/10], Step [300/1145], Loss: 0.9097549027204513\n",
      "Epoch [3/10], Step [400/1145], Loss: 0.9026881378889083\n",
      "Epoch [3/10], Step [500/1145], Loss: 0.931906316280365\n",
      "Epoch [3/10], Step [600/1145], Loss: 0.8968756234645844\n",
      "Epoch [3/10], Step [700/1145], Loss: 0.8933304113149643\n",
      "Epoch [3/10], Step [800/1145], Loss: 0.8693570387363434\n",
      "Epoch [3/10], Step [900/1145], Loss: 0.9219726502895356\n",
      "Epoch [3/10], Step [1000/1145], Loss: 0.8957352769374848\n",
      "Epoch [3/10], Step [1100/1145], Loss: 0.9138833647966385\n",
      "Epoch [4/10], Step [100/1145], Loss: 0.8667125904560089\n",
      "Epoch [4/10], Step [200/1145], Loss: 0.8659583073854447\n",
      "Epoch [4/10], Step [300/1145], Loss: 0.8620491874217987\n",
      "Epoch [4/10], Step [400/1145], Loss: 0.8479565393924713\n",
      "Epoch [4/10], Step [500/1145], Loss: 0.8628085994720459\n",
      "Epoch [4/10], Step [600/1145], Loss: 0.8387595361471176\n",
      "Epoch [4/10], Step [700/1145], Loss: 0.8573851460218429\n",
      "Epoch [4/10], Step [800/1145], Loss: 0.842053755223751\n",
      "Epoch [4/10], Step [900/1145], Loss: 0.8453026288747787\n",
      "Epoch [4/10], Step [1000/1145], Loss: 0.867058013677597\n",
      "Epoch [4/10], Step [1100/1145], Loss: 0.8741608038544655\n",
      "Epoch [5/10], Step [100/1145], Loss: 0.8256835705041885\n",
      "Epoch [5/10], Step [200/1145], Loss: 0.8386358314752579\n",
      "Epoch [5/10], Step [300/1145], Loss: 0.8328116255998611\n",
      "Epoch [5/10], Step [400/1145], Loss: 0.798481616973877\n",
      "Epoch [5/10], Step [500/1145], Loss: 0.86035828769207\n",
      "Epoch [5/10], Step [600/1145], Loss: 0.8419379132986069\n",
      "Epoch [5/10], Step [700/1145], Loss: 0.8277975544333458\n",
      "Epoch [5/10], Step [800/1145], Loss: 0.8183848488330842\n",
      "Epoch [5/10], Step [900/1145], Loss: 0.8061866825819015\n",
      "Epoch [5/10], Step [1000/1145], Loss: 0.8158362320065499\n",
      "Epoch [5/10], Step [1100/1145], Loss: 0.8330899524688721\n",
      "Epoch [6/10], Step [100/1145], Loss: 0.823932775259018\n",
      "Epoch [6/10], Step [200/1145], Loss: 0.8027787107229233\n",
      "Epoch [6/10], Step [300/1145], Loss: 0.80382936835289\n",
      "Epoch [6/10], Step [400/1145], Loss: 0.7966893419623375\n",
      "Epoch [6/10], Step [500/1145], Loss: 0.8115811562538147\n",
      "Epoch [6/10], Step [600/1145], Loss: 0.8182027518749238\n",
      "Epoch [6/10], Step [700/1145], Loss: 0.8017524561285972\n",
      "Epoch [6/10], Step [800/1145], Loss: 0.8201165109872818\n",
      "Epoch [6/10], Step [900/1145], Loss: 0.7696281403303147\n",
      "Epoch [6/10], Step [1000/1145], Loss: 0.8073912811279297\n",
      "Epoch [6/10], Step [1100/1145], Loss: 0.8051488134264946\n",
      "Epoch [7/10], Step [100/1145], Loss: 0.8149191129207611\n",
      "Epoch [7/10], Step [200/1145], Loss: 0.7967678120732308\n",
      "Epoch [7/10], Step [300/1145], Loss: 0.8015418326854706\n",
      "Epoch [7/10], Step [400/1145], Loss: 0.8107060322165489\n",
      "Epoch [7/10], Step [500/1145], Loss: 0.77846696048975\n",
      "Epoch [7/10], Step [600/1145], Loss: 0.7983745115995408\n",
      "Epoch [7/10], Step [700/1145], Loss: 0.7954140573740005\n",
      "Epoch [7/10], Step [800/1145], Loss: 0.7718401965498924\n",
      "Epoch [7/10], Step [900/1145], Loss: 0.7751770183444023\n",
      "Epoch [7/10], Step [1000/1145], Loss: 0.8005071181058884\n",
      "Epoch [7/10], Step [1100/1145], Loss: 0.7896382424235344\n",
      "Epoch [8/10], Step [100/1145], Loss: 0.7722360700368881\n",
      "Epoch [8/10], Step [200/1145], Loss: 0.7906487327814102\n",
      "Epoch [8/10], Step [300/1145], Loss: 0.7670147189497948\n",
      "Epoch [8/10], Step [400/1145], Loss: 0.7957967913150787\n",
      "Epoch [8/10], Step [500/1145], Loss: 0.7606779652833938\n",
      "Epoch [8/10], Step [600/1145], Loss: 0.785202129483223\n",
      "Epoch [8/10], Step [700/1145], Loss: 0.7806246247887612\n",
      "Epoch [8/10], Step [800/1145], Loss: 0.7555926856398583\n",
      "Epoch [8/10], Step [900/1145], Loss: 0.7959412556886672\n",
      "Epoch [8/10], Step [1000/1145], Loss: 0.7621423533558845\n",
      "Epoch [8/10], Step [1100/1145], Loss: 0.7750642278790474\n",
      "Epoch [9/10], Step [100/1145], Loss: 0.7814519986510277\n",
      "Epoch [9/10], Step [200/1145], Loss: 0.7567284691333771\n",
      "Epoch [9/10], Step [300/1145], Loss: 0.7768950656056404\n",
      "Epoch [9/10], Step [400/1145], Loss: 0.7701660352945328\n",
      "Epoch [9/10], Step [500/1145], Loss: 0.7581433975696563\n",
      "Epoch [9/10], Step [600/1145], Loss: 0.7581040582060814\n",
      "Epoch [9/10], Step [700/1145], Loss: 0.754020733833313\n",
      "Epoch [9/10], Step [800/1145], Loss: 0.7368072509765625\n",
      "Epoch [9/10], Step [900/1145], Loss: 0.7779052078723907\n",
      "Epoch [9/10], Step [1000/1145], Loss: 0.7678296852111817\n",
      "Epoch [9/10], Step [1100/1145], Loss: 0.7442925164103508\n",
      "Epoch [10/10], Step [100/1145], Loss: 0.7461781173944473\n",
      "Epoch [10/10], Step [200/1145], Loss: 0.7576707270741463\n",
      "Epoch [10/10], Step [300/1145], Loss: 0.7447008809447289\n",
      "Epoch [10/10], Step [400/1145], Loss: 0.7525786173343658\n",
      "Epoch [10/10], Step [500/1145], Loss: 0.7477633884549141\n",
      "Epoch [10/10], Step [600/1145], Loss: 0.7789566913247108\n",
      "Epoch [10/10], Step [700/1145], Loss: 0.7643930301070213\n",
      "Epoch [10/10], Step [800/1145], Loss: 0.7545279383659362\n",
      "Epoch [10/10], Step [900/1145], Loss: 0.758728999197483\n",
      "Epoch [10/10], Step [1000/1145], Loss: 0.739684474170208\n",
      "Epoch [10/10], Step [1100/1145], Loss: 0.7332085710763931\n",
      "Accuracy on the test set: 81.16548862937923%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),  # Resize images to fit AlexNet input size\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
    "])\n",
    "\n",
    "# Load SVHN dataset\n",
    "train_dataset = datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
    "test_dataset = datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load pre-trained AlexNet\n",
    "model = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "# Freeze the pre-trained weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier part of AlexNet for SVHN\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(256 * 6 * 6, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(4096, 10)  # Output layer for SVHN dataset\n",
    ")\n",
    "\n",
    "# Transfer the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the test set: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training VGG-16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/train_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dse313/miniconda3/envs/dse316_assignment/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.195\n",
      "[1,   200] loss: 2.053\n",
      "[1,   300] loss: 1.998\n",
      "[1,   400] loss: 1.940\n",
      "[1,   500] loss: 1.941\n",
      "[1,   600] loss: 1.897\n",
      "[1,   700] loss: 1.906\n",
      "[1,   800] loss: 1.876\n",
      "[1,   900] loss: 1.831\n",
      "[1,  1000] loss: 1.880\n",
      "[1,  1100] loss: 1.848\n",
      "[1,  1200] loss: 1.831\n",
      "[1,  1300] loss: 1.841\n",
      "[1,  1400] loss: 1.798\n",
      "[1,  1500] loss: 1.824\n",
      "[1,  1600] loss: 1.823\n",
      "[1,  1700] loss: 1.792\n",
      "[1,  1800] loss: 1.818\n",
      "[1,  1900] loss: 1.844\n",
      "[1,  2000] loss: 1.816\n",
      "[1,  2100] loss: 1.822\n",
      "[1,  2200] loss: 1.791\n",
      "[2,   100] loss: 1.794\n",
      "[2,   200] loss: 1.774\n",
      "[2,   300] loss: 1.783\n",
      "[2,   400] loss: 1.772\n",
      "[2,   500] loss: 1.754\n",
      "[2,   600] loss: 1.743\n",
      "[2,   700] loss: 1.788\n",
      "[2,   800] loss: 1.786\n",
      "[2,   900] loss: 1.779\n",
      "[2,  1000] loss: 1.754\n",
      "[2,  1100] loss: 1.779\n",
      "[2,  1200] loss: 1.755\n",
      "[2,  1300] loss: 1.788\n",
      "[2,  1400] loss: 1.791\n",
      "[2,  1500] loss: 1.762\n",
      "[2,  1600] loss: 1.741\n",
      "[2,  1700] loss: 1.747\n",
      "[2,  1800] loss: 1.765\n",
      "[2,  1900] loss: 1.745\n",
      "[2,  2000] loss: 1.759\n",
      "[2,  2100] loss: 1.725\n",
      "[2,  2200] loss: 1.770\n",
      "[3,   100] loss: 1.761\n",
      "[3,   200] loss: 1.791\n",
      "[3,   300] loss: 1.730\n",
      "[3,   400] loss: 1.758\n",
      "[3,   500] loss: 1.783\n",
      "[3,   600] loss: 1.715\n",
      "[3,   700] loss: 1.741\n",
      "[3,   800] loss: 1.728\n",
      "[3,   900] loss: 1.742\n",
      "[3,  1000] loss: 1.756\n",
      "[3,  1100] loss: 1.749\n",
      "[3,  1200] loss: 1.747\n",
      "[3,  1300] loss: 1.757\n",
      "[3,  1400] loss: 1.667\n",
      "[3,  1500] loss: 1.735\n",
      "[3,  1600] loss: 1.747\n",
      "[3,  1700] loss: 1.749\n",
      "[3,  1800] loss: 1.724\n",
      "[3,  1900] loss: 1.713\n",
      "[3,  2000] loss: 1.725\n",
      "[3,  2100] loss: 1.702\n",
      "[3,  2200] loss: 1.751\n",
      "[4,   100] loss: 1.725\n",
      "[4,   200] loss: 1.698\n",
      "[4,   300] loss: 1.747\n",
      "[4,   400] loss: 1.709\n",
      "[4,   500] loss: 1.706\n",
      "[4,   600] loss: 1.712\n",
      "[4,   700] loss: 1.732\n",
      "[4,   800] loss: 1.747\n",
      "[4,   900] loss: 1.721\n",
      "[4,  1000] loss: 1.703\n",
      "[4,  1100] loss: 1.707\n",
      "[4,  1200] loss: 1.697\n",
      "[4,  1300] loss: 1.725\n",
      "[4,  1400] loss: 1.760\n",
      "[4,  1500] loss: 1.708\n",
      "[4,  1600] loss: 1.737\n",
      "[4,  1700] loss: 1.698\n",
      "[4,  1800] loss: 1.708\n",
      "[4,  1900] loss: 1.777\n",
      "[4,  2000] loss: 1.724\n",
      "[4,  2100] loss: 1.689\n",
      "[4,  2200] loss: 1.725\n",
      "[5,   100] loss: 1.731\n",
      "[5,   200] loss: 1.754\n",
      "[5,   300] loss: 1.715\n",
      "[5,   400] loss: 1.706\n",
      "[5,   500] loss: 1.716\n",
      "[5,   600] loss: 1.732\n",
      "[5,   700] loss: 1.748\n",
      "[5,   800] loss: 1.706\n",
      "[5,   900] loss: 1.732\n",
      "[5,  1000] loss: 1.729\n",
      "[5,  1100] loss: 1.723\n",
      "[5,  1200] loss: 1.734\n",
      "[5,  1300] loss: 1.710\n",
      "[5,  1400] loss: 1.710\n",
      "[5,  1500] loss: 1.695\n",
      "[5,  1600] loss: 1.735\n",
      "[5,  1700] loss: 1.741\n",
      "[5,  1800] loss: 1.702\n",
      "[5,  1900] loss: 1.724\n",
      "[5,  2000] loss: 1.691\n",
      "[5,  2100] loss: 1.717\n",
      "[5,  2200] loss: 1.734\n",
      "[6,   100] loss: 1.721\n",
      "[6,   200] loss: 1.701\n",
      "[6,   300] loss: 1.670\n",
      "[6,   400] loss: 1.700\n",
      "[6,   500] loss: 1.717\n",
      "[6,   600] loss: 1.686\n",
      "[6,   700] loss: 1.707\n",
      "[6,   800] loss: 1.679\n",
      "[6,   900] loss: 1.721\n",
      "[6,  1000] loss: 1.714\n",
      "[6,  1100] loss: 1.708\n",
      "[6,  1200] loss: 1.678\n",
      "[6,  1300] loss: 1.711\n",
      "[6,  1400] loss: 1.717\n",
      "[6,  1500] loss: 1.719\n",
      "[6,  1600] loss: 1.766\n",
      "[6,  1700] loss: 1.702\n",
      "[6,  1800] loss: 1.710\n",
      "[6,  1900] loss: 1.737\n",
      "[6,  2000] loss: 1.683\n",
      "[6,  2100] loss: 1.690\n",
      "[6,  2200] loss: 1.737\n",
      "[7,   100] loss: 1.702\n",
      "[7,   200] loss: 1.677\n",
      "[7,   300] loss: 1.744\n",
      "[7,   400] loss: 1.702\n",
      "[7,   500] loss: 1.700\n",
      "[7,   600] loss: 1.699\n",
      "[7,   700] loss: 1.671\n",
      "[7,   800] loss: 1.689\n",
      "[7,   900] loss: 1.719\n",
      "[7,  1000] loss: 1.694\n",
      "[7,  1100] loss: 1.725\n",
      "[7,  1200] loss: 1.709\n",
      "[7,  1300] loss: 1.711\n",
      "[7,  1400] loss: 1.709\n",
      "[7,  1500] loss: 1.684\n",
      "[7,  1600] loss: 1.690\n",
      "[7,  1700] loss: 1.704\n",
      "[7,  1800] loss: 1.730\n",
      "[7,  1900] loss: 1.716\n",
      "[7,  2000] loss: 1.674\n",
      "[7,  2100] loss: 1.714\n",
      "[7,  2200] loss: 1.728\n",
      "[8,   100] loss: 1.705\n",
      "[8,   200] loss: 1.718\n",
      "[8,   300] loss: 1.689\n",
      "[8,   400] loss: 1.672\n",
      "[8,   500] loss: 1.709\n",
      "[8,   600] loss: 1.691\n",
      "[8,   700] loss: 1.684\n",
      "[8,   800] loss: 1.718\n",
      "[8,   900] loss: 1.639\n",
      "[8,  1000] loss: 1.698\n",
      "[8,  1100] loss: 1.675\n",
      "[8,  1200] loss: 1.699\n",
      "[8,  1300] loss: 1.705\n",
      "[8,  1400] loss: 1.679\n",
      "[8,  1500] loss: 1.710\n",
      "[8,  1600] loss: 1.688\n",
      "[8,  1700] loss: 1.683\n",
      "[8,  1800] loss: 1.714\n",
      "[8,  1900] loss: 1.696\n",
      "[8,  2000] loss: 1.701\n",
      "[8,  2100] loss: 1.691\n",
      "[8,  2200] loss: 1.668\n",
      "[9,   100] loss: 1.710\n",
      "[9,   200] loss: 1.677\n",
      "[9,   300] loss: 1.678\n",
      "[9,   400] loss: 1.684\n",
      "[9,   500] loss: 1.694\n",
      "[9,   600] loss: 1.679\n",
      "[9,   700] loss: 1.691\n",
      "[9,   800] loss: 1.701\n",
      "[9,   900] loss: 1.677\n",
      "[9,  1000] loss: 1.747\n",
      "[9,  1100] loss: 1.710\n",
      "[9,  1200] loss: 1.715\n",
      "[9,  1300] loss: 1.693\n",
      "[9,  1400] loss: 1.681\n",
      "[9,  1500] loss: 1.689\n",
      "[9,  1600] loss: 1.718\n",
      "[9,  1700] loss: 1.699\n",
      "[9,  1800] loss: 1.689\n",
      "[9,  1900] loss: 1.680\n",
      "[9,  2000] loss: 1.684\n",
      "[9,  2100] loss: 1.688\n",
      "[9,  2200] loss: 1.688\n",
      "[10,   100] loss: 1.698\n",
      "[10,   200] loss: 1.697\n",
      "[10,   300] loss: 1.691\n",
      "[10,   400] loss: 1.704\n",
      "[10,   500] loss: 1.700\n",
      "[10,   600] loss: 1.667\n",
      "[10,   700] loss: 1.711\n",
      "[10,   800] loss: 1.708\n",
      "[10,   900] loss: 1.693\n",
      "[10,  1000] loss: 1.658\n",
      "[10,  1100] loss: 1.709\n",
      "[10,  1200] loss: 1.653\n",
      "[10,  1300] loss: 1.703\n",
      "[10,  1400] loss: 1.681\n",
      "[10,  1500] loss: 1.702\n",
      "[10,  1600] loss: 1.651\n",
      "[10,  1700] loss: 1.678\n",
      "[10,  1800] loss: 1.690\n",
      "[10,  1900] loss: 1.718\n",
      "[10,  2000] loss: 1.698\n",
      "[10,  2100] loss: 1.633\n",
      "[10,  2200] loss: 1.682\n",
      "Finished Training\n",
      "Accuracy on the test set: 81.16548862937923%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the SVHN dataset\n",
    "trainset = torchvision.datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# Load pre-trained VGG-16\n",
    "vgg = vgg16(pretrained=True)\n",
    "# Freeze the pre-trained weights\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the last layer to fit the number of classes in SVHN (10)\n",
    "vgg.classifier[-1] = nn.Linear(4096, 10)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "vgg.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(vgg.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = vgg(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ResNet-18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n",
      "Epoch [1/10], Loss: 2.0302\n",
      "Epoch [2/10], Loss: 1.9585\n",
      "Epoch [3/10], Loss: 1.9520\n",
      "Epoch [4/10], Loss: 1.9532\n",
      "Epoch [5/10], Loss: 1.9507\n",
      "Epoch [6/10], Loss: 1.9486\n",
      "Epoch [7/10], Loss: 1.9477\n",
      "Epoch [8/10], Loss: 1.9505\n",
      "Epoch [9/10], Loss: 1.9482\n",
      "Epoch [10/10], Loss: 1.9476\n",
      "Accuracy on test set: 33.04%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load SVHN dataset\n",
    "train_dataset = datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
    "test_dataset = datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load pretrained ResNet-18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze pretrained weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the last fully connected layer for SVHN\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)  # 10 classes in SVHN\n",
    "\n",
    "# Send model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "# Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ResNet-50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dse313/miniconda3/envs/dse316_assignment/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.8382, Train Accuracy: 36.03%\n",
      "Epoch [2/10], Train Loss: 1.7005, Train Accuracy: 41.52%\n",
      "Epoch [3/10], Train Loss: 1.6560, Train Accuracy: 43.05%\n",
      "Epoch [4/10], Train Loss: 1.6263, Train Accuracy: 44.24%\n",
      "Epoch [5/10], Train Loss: 1.5950, Train Accuracy: 45.58%\n",
      "Epoch [6/10], Train Loss: 1.5788, Train Accuracy: 46.06%\n",
      "Epoch [7/10], Train Loss: 1.5642, Train Accuracy: 46.62%\n",
      "Epoch [8/10], Train Loss: 1.5455, Train Accuracy: 47.34%\n",
      "Epoch [9/10], Train Loss: 1.5333, Train Accuracy: 47.56%\n",
      "Epoch [10/10], Train Loss: 1.5209, Train Accuracy: 48.14%\n",
      "Test Accuracy: 48.78%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load SVHN dataset\n",
    "train_data = datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
    "test_data = datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final fully connected layer\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)  # SVHN has 10 classes\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss: {train_loss:.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ResNet-101 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dse313/miniconda3/envs/dse316_assignment/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.8318\n",
      "Epoch [2/10], Loss: 1.7034\n",
      "Epoch [3/10], Loss: 1.6706\n",
      "Epoch [4/10], Loss: 1.6475\n",
      "Epoch [5/10], Loss: 1.6172\n",
      "Epoch [6/10], Loss: 1.6018\n",
      "Epoch [7/10], Loss: 1.5826\n",
      "Epoch [8/10], Loss: 1.5768\n",
      "Epoch [9/10], Loss: 1.5573\n",
      "Epoch [10/10], Loss: 1.5475\n",
      "Accuracy on the test set: 47.69%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to fit ResNet input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
    "])\n",
    "\n",
    "# Load SVHN dataset\n",
    "train_dataset = datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
    "test_dataset = datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load pre-trained ResNet-101 model\n",
    "model = models.resnet101(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final fully connected layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)  # Change the output layer to fit SVHN dataset\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_pointnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
